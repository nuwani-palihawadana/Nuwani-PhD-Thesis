# Introduction {#sec-intro}

Forecasts are often contingent on a very long history of predictors. For example, when forecasting half-hourly electricity demand, it is common to use at least a week of historical half-hourly temperatures and other weather observations [@HF2010]. Similarly, when forecasting bore levels, rainfall data from up to thousand days earlier can impact the result [@Bakker2019] due to the complex flow dynamics of rainfall into aquifers.

On the other hand, in most of these applications, the relationships between the predictors and the response variable exhibit complex nonlinear patterns. For instance, the relationship between electricity demand and temperature is often nonlinear [@HF2010; @FH2012].

These examples suggest a possible nonlinear "*transfer function*" model of the form 
$$
 y_{t} = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots, \bm{x}_{t-p}, y_{t-1}, \dots, y_{t-k}) + \varepsilon_{t},
$$ {#eq-transferfun}
where $y_{t}$ is the observation of the response variable at time $t$, $\bm{x}_{t}$ is a vector of predictors at time $t$, and $\varepsilon_{t}$ is the random error. By including lagged values of $y_{t}$ along with the lagged predictors, we allow for any serial correlation in the data. However, it makes the resulting function hard to interpret. An alternative formulation is $$
 y_t = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots,\bm{x}_{t-p}) + g(\varepsilon_{t}, \varepsilon_{t-1},\dots,\varepsilon_{t-k}),
$$ which is more difficult to estimate, but makes it simpler to interpret the effect of the predictors on the response variable.

When applying the transfer function model to forecast lengthy time series with complex patterns, such as half-hourly electricity demand data and groundwater level data, the form of $f$ is nonlinear, involving complicated interactions, and with a high value of $p$.

Typically, the form of $f$ involves many ad hoc model choices. It is essentially impossible to estimate a $p$-dimensional function for large $p$ due to the curse of dimensionality [@Bellman57; @Stone82]. Instead, we normally impose some form of additivity, along with some low-order interactions.

For example, @FH2012 proposed a ***semi-parametric additive model*** to obtain short-term forecasts of the half-hourly electricity demand for power systems in the Australian National Electricity Market, where $f$ is assumed to be fully additive. The main objective behind the use of this proposed semi-parametric model is to allow nonparametric components in a regression-based modelling framework with serially correlated errors [@FH2012]. In this model, several calendar effects are used as linear predictors, while the temperature effects and the lagged effects of the response are modelled using nonparametric components estimated through regression splines.

Similarly, a ***distributed lag model*** was proposed by @Wood2017 to forecast daily death rate in Chicago using measurements of several air pollutants. In this model, the response variable is modelled via a sum of smooth functions of lagged predictor variables, which is quite similar in nature to the semi-parametric additive model used by @FH2012. However, unlike in @FH2012, @Wood2017 suggested to allow the smooth functions for lags of the same covariate to vary smoothly over lags, preventing large differences in estimated effects between adjacent lags. He proposed to use *tensor product smooths* to estimate the nonparametric functions related to these *distributed lag effects* of the predictors.

For more examples, @Ho2020 used semi-paramteric additive models to estimate ground-level $PM_{2.5}$ concentrations in Taiwan, while nonparametric additive models were utilised by @Ibrahim2023 for predicting census survey response rates. Furthermore, @Ravindra2019 provided a comprehensive review of the applications of additive models for environmental data, with a special focus on air pollution, climate change, and human health related studies.

While such models have been used to address problems including electricity demand, air quality related mortality rate and groundwater level forecasting etc. [@FH2012; @HF2010; @Wood2017; @Peterson2014; @Rajaee2019], there are still a number of unresolved issues in their applications. Firstly, even though nonparametric additive models act as a remedy to the curse of dimensionality as we discussed earlier, the estimation of the model is still challenging in a high-dimensional setting due to the large number of nonparametric components to be estimated. For example, in groundwater level forecasting, thousands of possible variables (probably lagged variables) are available as predictors. In this case, including all of them in the model, which will probably result in over-fitting, will not necessarily give the best possible model in terms of forecasting accuracy. Instead, a methodology should be found to select an optimal set of predictors that would result in a parsimonious model, while generating forecasts as accurate as possible. Secondly, there is a noticeable subjectivity in the selection of predictor variables for the model, where in most of the applications of interest we discussed above, the predictor choices are mainly based on empirical explorations or domain expertise.

There are a number of previous studies that have attempted to address the issue of variable selection in nonparametric/semi-parametric additive models to some extend, using various techniques. For example, @Huang2010 used a *Least Absolute Shrinkage and Selection Operator* [LASSO, @Tibshirani1996] based procedure for variable selection in nonparametric additive models, whereas @FH2012 used a straightforward backward elimination technique to achieve selection. Moreover, @Ibrahim2023 and @Hazimeh2023 used Mixed Integer Programming based methodologies to provide a solution to the *best subset selection* problem in nonparametric additive models.

In this thesis, however, we are interested in high-dimensional applications that exhibit complicated interactions among predictors (specially in the presence of large number of lagged variables), as well as correlated errors. In such a situation, *"index models"* seem to be useful for improving the flexibility of the broader class of nonparametric additive models [@Radchenko2015], while mitigating the difficulty of estimating a nonparametric component for each individual predictor. 
 
To our knowledge, no previous research has been done to look at how the predictor choices can be made more objective and principled in additive index models. Thus, the main focus of this thesis is to develop an optimal predictor selection methodology for nonparametric additive index models in a high-dimensional context, as a solution for the aforementioned issues.


## Background {#sec-background}

As discussed earlier, the estimation of nonparametric function $f$ (@eq-transferfun) becomes infeasible in high-dimensional settings (i.e. number of predictors is very large) due to curse of dimensionality. As a result, *nonparametric additive models* have been employed with growing popularity. Let $(y_{i}, \bm{x}_{i}), i = 1, \dots, n$, be independent and identically distributed (i.i.d) observations, and $\bm{x}_{i} = (x_{i1}, \dots, x_{ip})^{T}$ be a $p$-dimensional vector of predictor values. Then a nonparametric additive model can be written as
$$
 y_{i} = \sum_{j=1}^{p} {f_{j}(x_{ij})} + \varepsilon_{i}, \quad i = 1, \dots, n,
$$ {#eq-add}
where $f_{j}$'s are unknown functions (probably non-linear and smooth), and $\varepsilon_{i}$ is the random error [@Lian2012]. Even such an additivity condition is imposed, estimating the optimal predictive model will still be troublesome when $p$ is very large (probably even larger than the sample size $n$) due to over-fitting [@Lian2012]. Thus, it is natural to bring in the sparsity assumption, and assume that some of $f_{j}$'s are zero, which gives rise to the need of a variable selection method to differentiate between zero and non-zero components, while estimating the non-zero components [@Huang2010].

### Backward Elimination {#sec-backward}

In the problem of forecasting long-term peak electricity demand, @HF2010 used a stepwise procedure for variable selection through cross-validation. In the each half-hourly model fitted, the data is split into training and validation sets, and the predictors are selected into the model based on the Mean Squared Error (MSE) calculated for the validation set. Starting from the full model, the predictive power of each variable is evaluated by dropping one at a time. A predictor, the removal of which contributed to a decrease in the validation MSE, is omitted from the model in subsequent steps [@HF2010]. @FH2012 used a similar method except for the fact that they considered the Mean Absolute Percentage Error (MAPE) as the selection criterion. Therefore, both of these prior work use stepwise variable selection methodology based on out-of-sample forecasting performance.

### Penalisation Methods

According to @Huang2010, there are numerous penalised methods for variable selection and parameter estimation in high-dimensional settings, including the *bridge estimator* proposed by @Frank1993, the *Least Absolute Shrinkage and Selection Operator (LASSO)* by @Tibshirani1996, the *Smoothly Clipped Absolute Deviation Penalty* (SCAD) by @Fan2001, and the *Minimum Concave Penalty* (MCP) by @Zhang2010. Among them, we observe that the LASSO and the SCAD penalties are appearing popularly in literature.

@Tibshirani1996 introduced the regularisation method, ***LASSO***, for estimating linear models, which minimises the sum of squared residuals subject to the $\ell_{1}$ penalty on the coefficients. Assume the classical linear regression model $y_{i} = \sum_{j=1}^{p} {\beta_{j}x_{ij}} +\varepsilon_{i}$, fitted for the data $(y_{i}, \bm{x_{i}})$, $i = 1, \dots, n$, where $y_{i}$ is the response, $\bm{x_{i}} = (x_{i1}, \dots, x_{ip})^T$ is a $p$-dimensional vector of predictors, $\bm{\beta} = (\beta_{1}, \dots, \beta_{p})^{T}$ is the parameter vector corresponding to $\bm{x_{i}}$, and $\varepsilon_{i}$ is the random error. Then, the LASSO estimator, $\bm{\hat{\beta}}_{LASSO}$, can be obtained by
$$
 \bm{\hat{\beta}}_{LASSO} = \min_{\bm{\beta}}\left\{\left\lVert\bm{y} - \sum_{j=1}^{p} {\bm{x_{j}}\beta_{j}}\right\rVert_{2}^{2} + \lambda\sum_{j=1}^{p} {|\beta_{j}|}\right\},
$$
where $\bm{x_{j}} = \left (x_{1j}, \dots, x_{nj}\right )^{T}$, and $\lambda$ is a non-negative tuning parameter. The LASSO estimator reduces to the Ordinary Least Squares (OLS) estimator if $\lambda$ is equal to zero [@Konzen2016]. Due to the nature of the penalty applied, LASSO shrinks some of the coefficients towards zero, and sets the others exactly to zero, where the estimation of coefficients and variable selection are performed simultaneously [@Konzen2016].

While showing that the LASSO is not consistent for variable selection in certain situations, @Zou2006 introduced ***Adaptive Lasso*** (popularly known as "adaLASSO"); an extension of the LASSO method, which uses adaptive weights to penalise coefficients using the LASSO (i.e. $\ell_{1}$) penalty. Thus, the adaLASSO objective function can be written as
$$
 \bm{\hat{\beta}}_{adaLASSO} = \min_{\bm{\beta}}\left\{\left\lVert\bm{y} - \sum_{j=1}^{p} {\bm{x_{j}}\beta_{j}}\right\rVert_{2}^{2} + \lambda\sum_{j=1}^{p} {w_{j}|\beta_{j}|}\right\},
$$
where the vector of weights $\bm{w} = \left (w_{1}, \dots, w_{p} \right )^{T}$ is estimated by $\bm{\hat{w}} = 1/|\bm{\hat{\beta}}|^{\gamma}$ for $\gamma > 0$, which is a tuning parameter, and $\bm{\hat{\beta}}$ being any consistent estimator of $\bm{\beta}$ [@Zou2006].

@Yuan2006 considered the problem of selecting groups of variables, and discussed extensions of three variable selection and estimation methods namely, *LASSO* [@Tibshirani1996], *Least Angle Regression Selection* [LARS, @Efron2004], and *Non-negative Garrotte* [@Breiman1995]. Consider an $n$-dimensional response vector $\bm{y}$, and an $n \times p$ matrix of predictor values $\bm{X}$. Then the ***Group Lasso*** estimator of the coefficients vector $\bm{\beta}$ is obtained by minimising
$$
 \frac{1}{2}\left\lVert\bm{y} - \sum_{\ell=1}^{L} {\bm{X}_{\ell}\bm{\beta}_{\ell}}\right\rVert_{2}^{2} + \lambda\sum_{\ell=1}^{L} {\lVert\bm{\beta}_{\ell}\rVert}_{\bm{K}_{\ell}},
$$
where $\bm{X}_{\ell}$ is an $n \times p_{\ell}$ sub-matrix in $\bm{X}$ that corresponds to the $\ell^{th}$ group of predictors ($p_{\ell}$ is the number of predictors in $\ell^{th}$ group), $\bm{\beta}_{\ell}$ is the corresponding vector of coefficients, $\ell = 1, \dots, L$, $\lVert\bm{\beta}_{\ell}\rVert_{\bm{K}_{\ell}} = (\bm{\beta}_{\ell}' \bm{K}_{\ell} \bm{\beta}_{\ell})^{\frac{1}{2}}$ with $\bm{K}_{1}, \dots, \bm{K}_{L}$ being a set of given positive definite matrices, and $\lambda$ is a non-negative tuning parameter. Moreover, @Simon2013 proposed ***Sparse-Group Lasso***, which is a convex combination of general Lasso and Group Lasso methods, where the focus is on both "groupwise sparsity" (the number of groups with at least one nonzero coefficient), and "within group sparsity" (the number of nonzero coefficients within each nonzero group).

According to @Fan2001, a penalty function used in penalised least squares approaches should have three properties. Firstly, it should be singular at origin to generate a solution that is sparse. Secondly, it should fulfill certain conditions to be stable in model selection. Finally, it should be able to generate unbiased estimates for large coefficients via being bounded by a constant. They argued that all those three conditions are not satisfied by the penalisation methods such as the bridge regression [@Frank1993] and the LASSO [@Tibshirani1996]. Hence they proposed the ***SCAD*** penalty function, which is defined in terms of its first derivative as
$$
 p_{\lambda}'(\theta) = \lambda\left\{I(\theta\le\lambda) + \frac{(a\lambda - \theta)_{+}}{(a - 1)\lambda}I(\theta > \lambda)\right\},
$$
for some $a > 2$, and $\theta > 0$ [@Fan2001]. According to @Fan2001, the SCAD penalty function retains the favourable properties of both best subset selection and ridge regression, while having all three desired features, i.e., sparsity, stability, and unbiasedness.

Based on the above penalisation methods that are originally developed for linear models, @Huang2010 proposed a new penalisation method for variable selection in nonparametric additive model (@eq-add), named ***Adaptive Group Lasso***. They approximated $f_{j}$'s using normalised B-spline bases, so that a linear combination of B-spline basis functions is used to represent an individual nonparametric component $f_{j}$. The proposed method is a generalisation of Adaptive Lasso method [@Zou2006] to the Group Lasso method [@Yuan2006].

When the nonparametric additive model in @eq-add is considered, an obvious possibility is that some of the additive components (i.e. $f_{j}$'s) are being linear. For example, recall the electricity demand forecasting problem [@HF2010; @FH2012], where some of the calendar effects are included into the model as linear variables, whereas lagged temperature and lagged demand variables are included using nonlinear additive components. Such situations suggest the use of *semi-parametric partially linear additive models* that can be mathematically represented as
$$
 y_{i} = \sum_{j=1}^{p} {f_{j}(x_{ij})} + \sum_{k=1}^{q} {w_{ik}\beta_{k}} + \varepsilon_{i}, \quad i = 1, \dots, n,
$$
where $\bm{x}_{j}$'s, $j = 1, \dots, p$, are a set of predictors that enter the model as nonparametric components, whereas $\bm{w}_{k}$'s, $k = 1, \dots, q$ are another set of predictors that are included as linear components. While several studies have assumed that the number of nonparametric components are fixed, and performed variable selection only among the linear components of the model [@Lian2012; @Guo2013; @Liu2011], @Wang2014 introduced a methodology for selecting both linear and nonlinear components simultaneously, in the context of correlated, longitudinal data. They proposed the use of a ***Penalised Quadratic Inference Function (PQIF) with double SCAD penalties*** for variable selection and model estimation, where the correlation structure of the data was incorporated into the estimation method (see @Wang2014 for details).

### Time Series Aspect

It is worthwhile to briefly mention that there are extensions of the penalisation methods discussed above, which have specifically proposed to take the autocorrelation and lag structures in time series data into account.

@Wang2007 proposed an extension of the LASSO method for Regression with Autoregressive Error (REGAR) models. @Park2013 and @Konzen2016 proposed modifications to Adaptive Lasso method to incorporate the lag structures presented in Autoregressive Distributed Lag (ADL) models into the variable selection and estimation methodology. The ***Ordered Lasso*** was introduced by @Tibshirani2016 to deal with time-lagged regression problems, where we forecast the response value at time $t$ using the predictor values from $K$ previous time points, assuming that the magnitude of regression coefficients decreases as the lagged predictor moves away from time $t$.

However, it is important to note that all the models considered in the above time series related work are linear; none of them include nonparametric terms.

### Index Models {#sec-Index}

#### Single Index Model

The nonparametric additive model (@eq-add) estimates the relationship between the response and the predictors using a sum of univariate nonlinear functions corresponding to each individual predictor variable. Hence, it is incapable of handling the interactions among the predictors, which are ubiquitous in real-world problems [@Zhang2008].

As a remedy, the ***Single Index Model***, a generalisation of the linear regression model where the linear predictor is replaced by a semi-parametric component, is popularly being used in the literature [@Radchenko2015]. Let $y_{i}$ be the response, and $\bm{x}_{i}$ be a $p$-dimensional predictor vector. Then the single index model can be written as
$$
  y_{i} = g \left ( \bm{\alpha}^{T} \bm{x}_{i} \right ) + \varepsilon_{i}, \quad i = 1, \dots, n,
$$
where $\bm{\alpha}$ is a $p$-dimensional vector of unknown coefficients (i.e. parameters), $g$ is an unknown univariate function, and $\varepsilon_{i}$ is the random error [@Stoker1986; @Hardle1993]. The linear combination $\bm{\alpha}^{T} \bm{x}_{i}$ is called the *index*. Single index model is viewed as a viable alternative to the additive model since it offers more flexibility and interpretability [@Radchenko2015].

According to @Radchenko2015, single index models have widely been used in scenarios with fairly low and moderate dimensionality, where the corresponding estimation and variable selection techniques are not directly applicable to the high-dimensional setting. The error sum of squares of the model being non-convex with respect to index coefficients, is the main reason behind the existence of very limited number of methods in high-dimensional case [@Radchenko2015]. For an extensive summary of available methods, we refer to @Radchenko2015.

#### Multiple Index Models {#sec-multi-index}

**Projection Pursuit Regression**

@Friedman1981 introduced ***Projection Pursuit Regression (PPR)*** by extending the nonparametric additive model (@eq-add) to enable the modelling of interactions among predictor variables. On the other hand, PPR is an extension of the single index model to an *"additive index model"*, given by
$$
  y_{i} = \sum_{j=1}^{q} {g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i})} + \varepsilon_{i}, \quad i = 1, \dots, n,
$$
where $y_{i}$ is the response, $\bm{x}_{i}$ is a $p$-dimensional predictor vector, $\bm{\alpha}_{j} = \left ( \alpha_{j1}, \dots, \alpha_{jp} \right )^{T}, j = 1, \dots, q$ are $p$-dimensional projection vectors (or vectors of *"index coefficients"*), $g_{j}$'s are unknown univariate functions, and $\varepsilon_{i}$ is the random error.

Instead of estimating a single index, PPR estimates multiple indices and connects them to the response through a sum of univariate nonlinear functions. These indices are constructed through a *Projection Pursuit (PP)* [@Kruskal1969; @Friedman1974] algorithm, which is considered to be "interesting" low-dimensional projections of a high-dimensional feature space, obtained through the maximisation of an appropriate objective function or a "projection index" [@Huber1985].

According to @Zhang2008, PPR increases the power of additive models in high-dimensional settings, but it has two major drawbacks. Firstly, since PP increases the freedom of the additive model, it tends to overfit in a situation, where there are a lot of unimportant predictors. Secondly, the interpretation of the model estimated by PPR will be troublesome as many non-zero elements will be present in each projection vector $\bm{\alpha}_{j}$. To overcome these issues, @Zhang2008 introduced an $\ell_{1}$ regularised projection pursuit algorithm, where the resultant regression model is named as ***Sparse Projection Pursuit Regression*** (SpPPR). In SpPPR, an $\ell_{1}$ penalty (i.e. a LASSO penalty) on index coefficients is added to the cost function (the squared error) at each iteration of the PP, thereby performing variable selection and model estimation simultaneously. See @Zhang2008 for more details.

Although @Zhang2008 claimed that the SpPPR algorithm can detect important predictors even in a noisy data set, our experiments show that it is not particularly scalable for large data sets with both higher number of predictors and observations. \newline

**Group-wise Additive Index Model** 

Even though PPR introduces flexibility and the ability to model interactions among predictors into additive models, the indices obtained through PPR contain all the predictors at hand. Hence, even with a variable selection mechanism like SpPPR [@Zhang2008], PPR creates indices possibly by mixing heterogeneous variables in a single linear combination, making very little sense in terms of interpretability [@Masselot2022].

Typically, in many real-world problems, natural groupings can be identified in predictor variables. For example, naturally interacting variables can be grouped together, such as several lags of a predictor, weather related variables, and genes or proteins that are grouped by biological pathways in a biological study [@Masselot2022; @Wang2015]. 

This suggests the use of a ***Group-wise Additive Index Model (GAIM)***, which can be written as
$$
  y_{i} = \sum_{j = 1}^{p} g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \varepsilon_{i}, \quad i = 1, \dots, n, 
$$
where $y_{i}$ is the univariate response, $\bm{x}_{ij} \in \mathbb{R}^{l{j}}$, $j = 1, \dots, p$ are naturally occurring $p$ groups of predictors, which are $p$ non-overlapping subsets of $\bm{x}_{i}$ - the vector of all predictors, $\bm{\alpha}_{j}$ is a $l_{j}$-dimensional vector of index coefficients corresponding to the index $h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}$, $g_{j}$ is an unknown (possibly nonlinear) component function, and $\varepsilon_{i}$ is the random error, which is independent of $\bm{x}_{i}$ [@Wang2015-mp; @Masselot2022].

Since GAIM uses groups of predictors that are naturally or logically belonging together to construct indices, such derived indices will be more expressive and interpretable. However, at the same time, this introduces a certain level of subjectivity into the model formulation as different users can group the available predictors in different ways based on different logical reasoning. 


### Mathematical Optimisation for Variable Selection

#### Mathematical Optimisation

*Optimisation* plays a major role in both decision science and physical systems evaluation. ***Mathematical Optimisation*** or ***Mathematical Programming*** can be defined as the minimisation (or maximisation) of a function subject to restrictions on the unknowns/parameters of that function [@Nocedal2006]. Hence, a mathematical optimisation problem can be written as
$$
\begin{aligned}
  \min_{\bm{x}} \quad & f_{0}(\bm{x})\\
  \text{s.t.} \quad & f_{i}(\bm{x}) \le b_{i}, \quad i = 1, \dots, m
\end{aligned}
$$ {#eq-opt}
where the vector of unknowns or parameters of the problem is given by $\bm{x} = \left ( x_{1}, \dots, x_{n} \right )^{T}$, the *objective function* is denoted by $f_{0} : \mathbb{R}^{n} \rightarrow \mathbb{R}$, the *constraint functions* are given by $f_{i} : \mathbb{R}^{n} \rightarrow \mathbb{R}$, $i = 1, \dots, m$, and the bounds of the constraints are denoted by $\bm{b} = \left (b_{1}, \dots, b_{m} \right )^{T}$. A vector of values $\bm{x^{*}}$ that results in the smallest value for the objective function among all vectors that satisfy the stated constraints, is called the *optimal* value or the *solution* to the problem [@Boyd2004]. After mathematically formulating the optimisation problem as above (@eq-opt), an appropriate *optimisation algorithm* is used to obtain the solution $\bm{x^{*}}$ [@Nocedal2006].

Based on the form of the objective function and the constraints, various types of optimisation problems are identified.

An optimisation problem is known as a ***Linear Program*** (LP) when both the objective function and the constraints in @eq-opt (i.e. all $f_{i}$, $i = 0, \dots, m$) are linear. Hence, a LP can be written as
$$
\begin{aligned}
  \min_{\bm{x}} \quad & \bm{a_{0}}^{T}\bm{x}\\
  \text{s.t.} \quad & \bm{A}\bm{x} \le \bm{b},
\end{aligned}
$$ {#eq-lp}
where $\bm{x}$ is the vector that contains the parameters to be optimised, and $\bm{a_{0}} \in \mathbb{R}^{n}$ is the vector of coefficients of the objective function. The matrix of coefficients in the constraints is denoted by $\bm{A} \in \mathbb{R}^{m \times n}$, and $\bm{b}$ is the vector containing the upper bounds of the constraints. All LPs are *convex* optimisation problems [@Theusl2020].

The LP problem given in @eq-lp can be generalised to involve a quadratic term in the objective function, in which case it is called a ***Quadratic Program*** (QP). A QP can be written as
$$
\begin{aligned}
  \min_{\bm{x}} \quad & \frac{1}{2} \bm{x}^{T} \bm{Q_{0}} \bm{x} + \bm{a_{0}}^{T}\bm{x}\\
  \text{s.t.} \quad & \bm{A}\bm{x} \le \bm{b},
\end{aligned}
$$
where $\bm{Q_{0}} \in \mathbb{R}^{n \times n}$. Unless the matrix $\bm{Q_{0}}$ is positive semi-definite, a QP is non-convex [@Theusl2020].

If a linear objective function is minimised over a *convex cone*, such an optimisation problem is called a ***Conic Program*** (CP), which can be written as
$$
\begin{aligned}
  \min_{\bm{x}} \quad & \bm{a_{0}}^{T}\bm{x} \\
  \text{s.t.} \quad & \bm{A}\bm{x} + s = \bm{b}, \quad s \in \mathcal{K},
\end{aligned}
$$
where $\mathcal{K}$ denotes a nonempty closed convex cone. CPs are designed to model convex optimisation problems [@Theusl2020].

If we restrict some of the unknowns/parameters in an optimisation problem to take only integer values, then that optimisation problem is called a ***Mixed Integer Program*** (MIP). For example, if we constraint $x_{k} \in \mathbb{Z}$ for at least one $k$, $k \in \{1, \dots, n\}$ in the optimisation problem given by @eq-opt, then the optimisation problem becomes a MIP. If all the unknowns of an optimisation problem are constrained to be integers, such a problem is referred to as a pure ***Integer Program*** (IP), whereas if all the unknowns are bounded between zero and one (i.e. $\bm{x} \in \{ 0, 1 \}^{n}$), the optimisation problem is referred to as a ***Binary (Integer) Program*** [@Theusl2020]. MIPs are hard to solve as they are non-convex due to the integer constraints. However, a growth in the number of commercial as well as non-commercial MIP solvers has made it possible to solve MIP problems conveniently and directly.

#### Variable Selection

Mathematical optimisation is fundamentally important in statistics, as many statistical problems including regression, classification, and other types of estimation/approximation problems can be re-interpreted as optimisation problems [@Theusl2020]. Thus, the problem of variable selection - one of the prolonged interests of statisticians, has also benefited from using optimisation concepts, particularly MIP and convex optimisation, in the recent past.

For example, @Bertsimas2016 used a mixed integer optimisation procedure to solve the classical best subset selection problem in a linear regression. They developed a discrete optimisation method by extending modern first-order continuous optimisation techniques. The method can produce near-optimal solutions that would serve as warm starts for a MIP algorithm, which would choose the best $k$ features out of $p$ predictors. Similarly, @Hazimeh2020 developed fast and efficient algorithms based on coordinate descent and local combinatorial optimisation to solve the same best subset selection (or $\ell_{0}$-regularised least squares) problem through re-formulating local combinatorial search problems as structured MIPs.

Furthermore, @Hazimeh2023 proposed a group-wise variable selection methodology, based on discrete mathematical optimisation, which is applicable to both $\ell_{0}$-regularised linear regression and nonparametric additive models in a high-dimensional setting. They formulated the group $\ell_{0}$-based estimation problem as a *Mixed Integer Second Order Cone Program (MISOCP)*, and proposed a new customised Branch-and-Bound (BnB) algorithm [@Land1960; @Little1963] to obtain the global optimal solution to the MISOCP.

Through the study of above literature, we noticed that the mathematical optimisation based algorithms reduce computational cost of variable selection procedures in high-dimensional settings. This is largely due to the availability of efficient commercial solvers such as *Gurobi* and *CPLEX*. This motivated us to focus on a mathematical optimisation based procedure for developing our variable selection methodology.


## Motivation and Objectives {#sec-objectives}

In this thesis, our aim is to reduce the subjectivity induced by personal judgment or domain expertise in model estimation. Hence, we propose a methodology that injects more objectivity into the estimation of multiple index models by algorithmically grouping predictors into indices, resulting in a model with a higher predictive accuracy.

Hence, the **first** objective of this thesis is to develop an objective and a principled methodology for optimal predictor selection in high-dimensional nonparametric additive index models. This will potentially minimise the necessity to utilise human expertise or prior knowledge in the process. Moreover, we also plan to automate the proposed variable selection methodology and the modelling framework using an open source R package, as a part of this research objective. This first objective will be addressed is **Chapter 2** of the thesis.

Furthermore, in a forecasting methodology, it will be necessary to produce prediction intervals. However, these will probably be underestimated due to the variable selection process. Therefore, the **second** objective of the thesis is to come up with a method to produce prediction intervals, while addressing the aforementioned problem of underestimation. **Chapter 3** of the thesis will focus on addressing this objective.

Forecasting electricity demand is crucial for planning and management of power systems.
Overestimating electricity demand will result in unnecessary investments in the construction
of power facilities, and excessive energy purchases; underestimating it will lead to inadequate preparation and unmet demand. Hence, electric utilities and regulators rely heavily on electricity demand forecasts [@FH2012]. Hence, as the **third** objective of the thesis, we plan to apply the proposed modelling framework for electricity demand data, with the aim of estimating a forecasting model that produces accurate forecasts. The third objective will form the **Chapter 4** of the thesis.

The aforementioned research objectives of the thesis are summarised below.

**Research Objectives:**

1.  Develop an objective and a principled methodology for optimal predictor selection in high-dimensional nonparametric additive index models with serially correlated errors, and automate the modelling framework using a comprehensive, user-friendly R package.
2.  Produce prediction intervals, while addressing the issue of the coverage of the prediction intervals being underestimated as a result of the variable selection process.
3.  Utilise sparse nonparametric models as a forecasting tool in application to electricity demand data, and obtain estimates of forecast uncertainty.
